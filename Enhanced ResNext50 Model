import torch
import torch.nn as nn
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
from tensorflow.keras.models import load_model
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

# Swish activation function
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

# Stochastic Pooling layer
class StochasticPooling2D(nn.Module):
    def forward(self, x):
        N, C, H, W = x.size()
        pool_size = 2
        out_H, out_W = H // pool_size, W // pool_size
        out = torch.zeros(N, C, out_H, out_W).to(x.device)

        for i in range(out_H):
            for j in range(out_W):
                region = x[:, :, i*2:(i+1)*2, j*2:(j+1)*2]
                probs = F.softmax(region.reshape(N, C, -1), dim=-1)
                sampled = torch.multinomial(probs, 1).squeeze(-1)
                out[:, :, i, j] = torch.gather(region.reshape(N, C, -1), 2, sampled.unsqueeze(-1)).squeeze(-1)
        return out

# Cardinality Block with Swish
class CardinalityBlock(nn.Module):
    def __init__(self, in_channels, out_channels, cardinality=32, stride=1, identity_downsample=None):
        super(CardinalityBlock, self).__init__()
        self.C = cardinality
        self.expansion = 2
        self.downsample = identity_downsample
        self.branches = nn.ModuleList()

        for _ in range(self.C):
            branch = nn.Sequential(
                nn.Conv2d(in_channels, out_channels//self.C, kernel_size=1, stride=1, padding=0),
                nn.BatchNorm2d(out_channels//self.C),
                Swish(),

                nn.Conv2d(out_channels//self.C, out_channels//self.C, kernel_size=3, stride=stride, padding=1),
                nn.BatchNorm2d(out_channels//self.C),
                Swish(),

                nn.Conv2d(out_channels//self.C, (out_channels//self.C)*self.expansion, kernel_size=1),
                nn.BatchNorm2d((out_channels//self.C)*self.expansion)
            )
            self.branches.append(branch)

        self.swish = Swish()

    def forward(self, x):
        identity = x
        out = torch.cat([branch(x) for branch in self.branches], dim=1)

        if self.downsample is not None:
            identity = self.downsample(identity)
        
        out += identity
        return self.swish(out)

# Main Enhanced ResNeXt-50 Model
class EnhancedResNeXt50(nn.Module):
    def __init__(self, num_classes=3):
        super(EnhancedResNeXt50, self).__init__()
        self.in_channels = 64
        self.swish = Swish()
        self.stochastic_pool = StochasticPooling2D()

        # Stem
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)

        # Stochastic pool replaces max pool
        self.pool = self.stochastic_pool

        # ResNeXt blocks
        self.layer1 = self.make_layer(128, 3, stride=1)
        self.layer2 = self.make_layer(256, 4, stride=2)
        self.layer3 = self.make_layer(512, 6, stride=2)
        self.layer4 = self.make_layer(1024, 3, stride=2)

        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(1024 * 2, num_classes)

    def make_layer(self, out_channels, blocks, stride):
        identity_downsample = nn.Sequential(
            nn.Conv2d(self.in_channels, out_channels*2, kernel_size=1, stride=stride),
            nn.BatchNorm2d(out_channels*2)
        )
        layers = [CardinalityBlock(self.in_channels, out_channels, stride=stride, identity_downsample=identity_downsample)]
        self.in_channels = out_channels * 2
        for _ in range(1, blocks):
            layers.append(CardinalityBlock(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.swish(self.bn1(self.conv1(x)))
        x = self.pool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

# Training function
def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs=25, device='cuda'):
    best_acc = 0.0
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        
        # Each epoch has training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data
            for inputs, labels in tqdm(dataloaders[phase]):
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # Backpropagation
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train' and scheduler is not None:
                scheduler.step()

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Save history
            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())

            # Save best model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), 'best_enhanced_resnext50.pth')

    print(f"Best validation accuracy: {best_acc:.4f}")
    return history

# Evaluation function
def evaluate_model(model, test_loader, device='cuda'):
    model.eval()  # Set the model to evaluation mode
    all_preds = []
    all_labels = []

    # Make predictions on the test set
    with torch.no_grad():  # No need to compute gradients during evaluation
        for inputs, labels in test_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            all_preds.extend(preds.cpu().numpy())  # Move to CPU for numpy operations
            all_labels.extend(labels.cpu().numpy())

    # Convert to numpy arrays for metrics calculation
    y_pred_classes = np.array(all_preds)
    y_true = np.array(all_labels)

    # Calculate accuracy
    accuracy = accuracy_score(y_true, y_pred_classes)
    print(f"Accuracy: {accuracy * 100:.2f}%")

    # Precision, Recall, F1 Score
    report = classification_report(y_true, y_pred_classes)
    print(report)

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred_classes)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Class1", "Class2", "Class3"], yticklabels=["Class1", "Class2", "Class3"])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# Example Usage:
# Assuming you have your DataLoader for training and testing
# train_loader, val_loader, test_loader = ... (Your DataLoader for train, validation, and test)

# Initialize model
model = EnhancedResNeXt50(num_classes=3)

# Define loss, optimizer, and scheduler
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

# Train model
history = train_model(model, {'train': train_loader, 'val': val_loader}, criterion, optimizer, scheduler, num_epochs=25, device='cuda')

# Evaluate the model
evaluate_model(model, test_loader, device='cuda')

