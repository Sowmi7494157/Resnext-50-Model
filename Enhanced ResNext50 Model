import torch
import torch.nn as nn
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
from tensorflow.keras.models import load_model
from mealpy.swarm_based import CSO  # Cat Swarm Optimization
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import roc_auc_score, cohen_kappa_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import label_binarize

# Swish activation function
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

# Stochastic Pooling layer
class StochasticPooling2D(nn.Module):
    def forward(self, x):
        N, C, H, W = x.size()
        pool_size = 2
        out_H, out_W = H // pool_size, W // pool_size
        out = torch.zeros(N, C, out_H, out_W).to(x.device)

        for i in range(out_H):
            for j in range(out_W):
                region = x[:, :, i*2:(i+1)*2, j*2:(j+1)*2]
                probs = F.softmax(region.reshape(N, C, -1), dim=-1)
                sampled = torch.multinomial(probs, 1).squeeze(-1)
                out[:, :, i, j] = torch.gather(region.reshape(N, C, -1), 2, sampled.unsqueeze(-1)).squeeze(-1)
        return out

# Cardinality Block with Swish
class CardinalityBlock(nn.Module):
    def __init__(self, in_channels, out_channels, cardinality=32, stride=1, identity_downsample=None):
        super(CardinalityBlock, self).__init__()
        self.C = cardinality
        self.expansion = 2
        self.downsample = identity_downsample
        self.branches = nn.ModuleList()

        for _ in range(self.C):
            branch = nn.Sequential(
                nn.Conv2d(in_channels, out_channels//self.C, kernel_size=1, stride=1, padding=0),
                nn.BatchNorm2d(out_channels//self.C),
                Swish(),

                nn.Conv2d(out_channels//self.C, out_channels//self.C, kernel_size=3, stride=stride, padding=1),
                nn.BatchNorm2d(out_channels//self.C),
                Swish(),

                nn.Conv2d(out_channels//self.C, (out_channels//self.C)*self.expansion, kernel_size=1),
                nn.BatchNorm2d((out_channels//self.C)*self.expansion)
            )
            self.branches.append(branch)

        self.swish = Swish()

    def forward(self, x):
        identity = x
        out = torch.cat([branch(x) for branch in self.branches], dim=1)

        if self.downsample is not None:
            identity = self.downsample(identity)
        
        out += identity
        return self.swish(out)

# Main Enhanced ResNeXt-50 Model
class EnhancedResNeXt50(nn.Module):
    def __init__(self, num_classes=3):
        super(EnhancedResNeXt50, self).__init__()
        self.in_channels = 64
        self.swish = Swish()
        self.stochastic_pool = StochasticPooling2D()

        # Stem
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)

        # Stochastic pool replaces max pool
        self.pool = self.stochastic_pool

        # ResNeXt blocks
        self.layer1 = self.make_layer(128, 3, stride=1)
        self.layer2 = self.make_layer(256, 4, stride=2)
        self.layer3 = self.make_layer(512, 6, stride=2)
        self.layer4 = self.make_layer(1024, 3, stride=2)

        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(1024 * 2, num_classes)

    def make_layer(self, out_channels, blocks, stride):
        identity_downsample = nn.Sequential(
            nn.Conv2d(self.in_channels, out_channels*2, kernel_size=1, stride=stride),
            nn.BatchNorm2d(out_channels*2)
        )
        layers = [CardinalityBlock(self.in_channels, out_channels, stride=stride, identity_downsample=identity_downsample)]
        self.in_channels = out_channels * 2
        for _ in range(1, blocks):
            layers.append(CardinalityBlock(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.swish(self.bn1(self.conv1(x)))
        x = self.pool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

# Training function
def train_model(model, dataloaders, criterion, optimizer, scheduler=None, num_epochs=100, device='cuda'):
    best_acc = 0.0
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in tqdm(dataloaders[phase]):
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train' and scheduler is not None:
                scheduler.step()

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            if phase == 'train':
                history['train_loss'].append(epoch_loss)
                history['train_acc'].append(epoch_acc.item())
            else:
                history['val_loss'].append(epoch_loss)
                history['val_acc'].append(epoch_acc.item())

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), 'best_enhanced_resnext50.pth')

    print(f"Best validation accuracy: {best_acc:.4f}")
    return history

# CSO Objective function for hyperparameter tuning
def objective_function(hyperparams):
    lr = hyperparams[0]
    weight_decay = hyperparams[1]
    
    print(f"Testing with lr: {lr:.6f}, weight_decay: {weight_decay:.6f}")

    model = EnhancedResNeXt50(num_classes=3).to('cuda')
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    
    # Run 2 epochs to evaluate quickly
    history = train_model(
        model,
        {'train': train_loader, 'val': val_loader},
        criterion,
        optimizer,
        scheduler=None,
        num_epochs=2,
        device='cuda'
    )
    
    val_loss = history['val_loss'][-1]
    print(f"Validation loss: {val_loss:.4f}")
    return val_loss
def evaluate_model_metrics(model, dataloader, num_classes, device='cuda'):
    model.eval()
    all_preds = []
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # Convert to numpy arrays
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # Metrics
    acc = accuracy_score(all_labels, all_preds)
    kappa = cohen_kappa_score(all_labels, all_preds)
    conf_matrix = confusion_matrix(all_labels, all_preds)

    # ROC-AUC (one-vs-rest for multi-class)
    all_labels_bin = label_binarize(all_labels, classes=list(range(num_classes)))
    roc_auc = roc_auc_score(all_labels_bin, all_probs, multi_class='ovr')

    print(f"\nAccuracy: {acc:.4f}")
    print(f"Cohen's Kappa: {kappa:.4f}")
    print(f"ROC-AUC Score: {roc_auc:.4f}")
    print("\nClassification Report:")
    print(classification_report(all_labels, all_preds))

    # Confusion Matrix Plot
    plt.figure(figsize=(6, 5))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Greens')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.tight_layout()
    plt.show()

evaluate_model_metrics(final_model, val_loader, num_classes=3, device='cuda')

# Define CSO problem
problem = {
    "fit_func": objective_function,
    "lb": [1e-5, 0.0],      # [learning rate lower bound, weight_decay lower bound]
    "ub": [1e-3, 1e-2],     # [learning rate upper bound, weight_decay upper bound]
    "minmax": "min",        # Minimize validation loss
}

# Run CSO optimization
model_cso = CSO.OriginalCSO(epoch=10, pop_size=5)  # 10 iterations, 5 cats (population)
best_position, best_val_loss = model_cso.solve(problem)

print(f"Best Hyperparameters Found: LR={best_position[0]:.6f}, Weight Decay={best_position[1]:.6f}")

# After CSO: Train your final model with best hyperparameters
final_model = EnhancedResNeXt50(num_classes=3).to('cuda')
final_criterion = nn.CrossEntropyLoss()
final_optimizer = optim.Adam(final_model.parameters(), lr=best_position[0], weight_decay=best_position[1])
final_scheduler = optim.lr_scheduler.StepLR(final_optimizer, step_size=7, gamma=0.1)

# Final Training
history = train_model(final_model, {'train': train_loader, 'val': val_loader}, final_criterion, final_optimizer, final_scheduler, num_epochs=100, device='cuda')
