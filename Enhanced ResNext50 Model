import torch
import torch.nn as nn
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import torch.nn.functional as F
import torch.optim as optim
from tqdm import tqdm
from tensorflow.keras.models import load_model
from mealpy.swarm_based import CSO  # Cat Swarm Optimization
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.metrics import roc_auc_score, cohen_kappa_score
from sklearn.preprocessing import label_binarize
from scipy.stats import ttest_rel

# Swish activation function
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

# Stochastic Pooling layer
class StochasticPooling2D(nn.Module):
    def forward(self, x):
        N, C, H, W = x.size()
        pool_size = 2
        out_H, out_W = H // pool_size, W // pool_size
        out = torch.zeros(N, C, out_H, out_W).to(x.device)

        for i in range(out_H):
            for j in range(out_W):
                region = x[:, :, i*2:(i+1)*2, j*2:(j+1)*2]
                probs = F.softmax(region.reshape(N, C, -1), dim=-1)
                sampled = torch.multinomial(probs, 1).squeeze(-1)
                out[:, :, i, j] = torch.gather(region.reshape(N, C, -1), 2, sampled.unsqueeze(-1)).squeeze(-1)
        return out

# Cardinality Block with Swish
class CardinalityBlock(nn.Module):
    def __init__(self, in_channels, out_channels, cardinality=32, stride=1, identity_downsample=None):
        super(CardinalityBlock, self).__init__()
        self.C = cardinality
        self.expansion = 2
        self.downsample = identity_downsample
        self.branches = nn.ModuleList()

        for _ in range(self.C):
            branch = nn.Sequential(
                nn.Conv2d(in_channels, out_channels//self.C, kernel_size=1, stride=1, padding=0),
                nn.BatchNorm2d(out_channels//self.C),
                Swish(),

                nn.Conv2d(out_channels//self.C, out_channels//self.C, kernel_size=3, stride=stride, padding=1),
                nn.BatchNorm2d(out_channels//self.C),
                Swish(),

                nn.Conv2d(out_channels//self.C, (out_channels//self.C)*self.expansion, kernel_size=1),
                nn.BatchNorm2d((out_channels//self.C)*self.expansion)
            )
            self.branches.append(branch)

        self.swish = Swish()

    def forward(self, x):
        identity = x
        out = torch.cat([branch(x) for branch in self.branches], dim=1)

        if self.downsample is not None:
            identity = self.downsample(identity)

        out += identity
        return self.swish(out)

# Main Enhanced ResNeXt-50 Model
class EnhancedResNeXt50(nn.Module):
    def __init__(self, num_classes=3):
        super(EnhancedResNeXt50, self).__init__()
        self.in_channels = 64
        self.swish = Swish()
        self.stochastic_pool = StochasticPooling2D()

        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool = self.stochastic_pool

        self.layer1 = self.make_layer(128, 3, stride=1)
        self.layer2 = self.make_layer(256, 4, stride=2)
        self.layer3 = self.make_layer(512, 6, stride=2)
        self.layer4 = self.make_layer(1024, 3, stride=2)

        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.classifier = nn.Linear(1024 * 2, num_classes)

    def make_layer(self, out_channels, blocks, stride):
        identity_downsample = nn.Sequential(
            nn.Conv2d(self.in_channels, out_channels*2, kernel_size=1, stride=stride),
            nn.BatchNorm2d(out_channels*2)
        )
        layers = [CardinalityBlock(self.in_channels, out_channels, stride=stride, identity_downsample=identity_downsample)]
        self.in_channels = out_channels * 2
        for _ in range(1, blocks):
            layers.append(CardinalityBlock(self.in_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.swish(self.bn1(self.conv1(x)))
        x = self.pool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

# Training function
def train_model(model, dataloaders, criterion, optimizer, scheduler=None, num_epochs=500, device='cuda'):
    best_acc = 0.0
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()

            running_loss = 0.0
            running_corrects = 0

            for inputs, labels in tqdm(dataloaders[phase]):
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train' and scheduler is not None:
                scheduler.step()

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            history[f'{phase}_loss'].append(epoch_loss)
            history[f'{phase}_acc'].append(epoch_acc.item())

            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), 'best_enhanced_resnext50.pth')

    print(f"Best validation accuracy: {best_acc:.4f}")
    return history

# Evaluation function
def evaluate_model_metrics(model, dataloader, num_classes, device='cuda'):
    model.eval()
    all_preds, all_probs, all_labels = [], [], []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            probs = F.softmax(outputs, dim=1)
            _, preds = torch.max(probs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds, all_labels, all_probs = map(np.array, (all_preds, all_labels, all_probs))
    acc = accuracy_score(all_labels, all_preds)
    kappa = cohen_kappa_score(all_labels, all_preds)
    roc_auc = roc_auc_score(label_binarize(all_labels, classes=list(range(num_classes))), all_probs, multi_class='ovr')

    print(f"\nAccuracy: {acc:.4f}\nCohen's Kappa: {kappa:.4f}\nROC-AUC Score: {roc_auc:.4f}")
    print("\nClassification Report:\n", classification_report(all_labels, all_preds))

    plt.figure(figsize=(6, 5))
    sns.heatmap(confusion_matrix(all_labels, all_preds), annot=True, fmt='d', cmap='Greens')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.tight_layout()
    plt.show()

    return acc

# Paired t-test evaluation for 10 runs
acc_scores = []
for i in range(10):
    final_model = EnhancedResNeXt50(num_classes=3).to('cuda')
    final_optimizer = optim.Adam(final_model.parameters(), lr=1e-4, weight_decay=1e-3)
    final_scheduler = optim.lr_scheduler.StepLR(final_optimizer, step_size=7, gamma=0.1)
    train_model(final_model, {'train': train_loader, 'val': val_loader}, nn.CrossEntropyLoss(), final_optimizer, final_scheduler, num_epochs=500)
    acc_scores.append(evaluate_model_metrics(final_model, val_loader, num_classes=3))

# Compare accuracy using paired t-test (example: first 5 runs vs. last 5 runs)
t_stat, p_val = ttest_rel(acc_scores[:5], acc_scores[5:])
print(f"\nPaired t-test result: t-statistic={t_stat:.4f}, p-value={p_val:.4f}")

# CSO optimization
problem = {
    "fit_func": objective_function,
    "lb": [1e-5, 0.0],
    "ub": [1e-3, 1e-2],
    "minmax": "min",
    "log_to": None,
}
model_cso = CSO.OriginalCSO(epoch=10, pop_size=5)
best_position, best_val_loss = model_cso.solve(problem)

print(f"Best Hyperparameters Found: LR={best_position[0]:.6f}, Weight Decay={best_position[1]:.6f}")
